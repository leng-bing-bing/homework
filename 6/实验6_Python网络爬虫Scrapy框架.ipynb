{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验目的：\n",
    "1. 了解Scrapy爬虫框架；\n",
    "2. 掌握Scrapy爬虫的使用步骤；\n",
    "3. 了解Scrapy爬虫提取信息的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验要求：\n",
    "1. 使用Scrapy爬取某一城市一周的天气预报；\n",
    "2. 使用Scrapy爬取上交所和深交所所有股票的名称和交易信息；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络爬虫之Scrapy框架\n",
    "\n",
    "- Scrapy是一个快速、功能强大的网络爬虫框架，不是一个函数功能库；\n",
    "- 爬虫框架是实现爬虫功能的一个软件结构和功能组件集合，是一个半成品，它能够帮助用户实现专业网络爬虫。\n",
    "\n",
    "在win平台安装Scrapy，以管理员身份运行cmd，执行**pip install scrapy**；安装完成之后，执行**scrapy -h**进行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy爬虫框架 5+2（中间件）  结构：\n",
    "- Engine\n",
    "    - 控制所有模块之间的数据流\n",
    "    - 根据条件触发事件\n",
    "    - 不需要用户修改\n",
    "- Downloader\n",
    "    - 根据请求下载网页\n",
    "    - 不需要用户修改\n",
    "- Scheduler\n",
    "    - 对所有爬取请求进行调度管理\n",
    "    - 不需要用户修改\n",
    "- Downloader Middleware\n",
    "    - 目的：实施Engine,Scheduler和Downloader之间进行用户可配置的控制\n",
    "    - 功能：修改、丢弃、新增请求或相应\n",
    "    - 用户可以编写配置代码\n",
    "    \n",
    "- Spider\n",
    "    - 解析Downloader返回的响应(Response)\n",
    "    - 产生爬取项（scraped item）\n",
    "    - 产生额外的爬取请求(Request)\n",
    "    - 需要用户编写配置代码\n",
    "\n",
    "- Item Pipelines\n",
    "    - 以流水线方式处理Spider产生的爬取项\n",
    "    - 由一组操作顺序组成，类似流水线，每个操作是一个Item Pipeline类型\n",
    "    - 可能操作包括：清理、检验和查重爬取项中的HTML数据、将数据存储到数据库\n",
    "    - 需要用户编写配置代码\n",
    "    \n",
    "- Spider Middleware\n",
    "    - 目的：对请求和爬取项的再处理\n",
    "    - 功能：修改、丢弃、新增请求或爬取项\n",
    "    - 用户可以编写配置代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy常用命令\n",
    "\n",
    "命令  | 说明  | 格式 \n",
    "---|---|---|\n",
    "  genspider   | 使用之前定义的模板创建一个新的爬虫  | scrapy genspider [options] <name> <domain>\n",
    "  settings    | 获取爬虫配置信息 | scrapy settings [options]  \n",
    "  shell      | 启动URL调试命令行 | scrapy shell [url]\n",
    "  startproject  | 创建一个新工程   | scrapy startproject <name>[dir]\n",
    "  crawl  | 运行一个爬虫  | scrapy crawl <spider>\n",
    "  list   | 列出工程中所有爬虫 | scrapy list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy 爬虫的命令行逻辑\n",
    "- Scrapy采用命令行创建和运行爬虫；\n",
    "- 命令行（不是图形界面）更容易自动化，适合脚本控制；\n",
    "- 本质上，Scrapy是给程序员用的，功能更重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy爬虫的使用步骤\n",
    "1. 创建一个工程和Spider模板\n",
    "2. 编写Spider\n",
    "3. 编写Item Pipeline\n",
    "4. 优化配置策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy爬虫演示实例\n",
    "\n",
    "- 演示HTML页面地址：http://python123.io/ws/demo.html\n",
    "\n",
    "**步骤1**：建立一个scrapy爬虫工程\n",
    "- 选取一个目录（如：C:\\Users\\Desktop\\）,然后在该目录下执行如下命令：\n",
    "**>scrapy startproject python123demo**\n",
    "- 生成的项目 python123demo 里面包含一个目录 python123demo（模块） 和一个文件 scrapy.cfg（部署scrapy爬虫的配置文件）\n",
    "- 内层目录python123demo里面包含以下目录和文件：\n",
    "   - **\\__pycache\\__**:缓存目录，无需修改\n",
    "   - **spiders**：Spiders代码模块目录（继承类），里面包含：\n",
    "     - **\\__init\\__**：初始文件，无需修改\n",
    "     - **\\__pycache\\__**:缓存目录，无需修改\n",
    "   - **\\__init\\__**：初始化脚本\n",
    "   - **items**：items代码模块（继承类）\n",
    "   - **middlewares**：Middlewares代码模板（继承类）\n",
    "   - **pipelines**：Pipelines代码模板（继承类）\n",
    "   - **settings**：Scrapy爬虫的配置文件  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤2**：在工程中产生一个Scrapy爬虫\n",
    " - 进入工程目录python123demo，然后执行如下命令：**>scrapy genspider demo python123.io**\n",
    " - 该命令作用：\n",
    "   - 在工程目录下生成一个名称为demo的spider；\n",
    "   - 在spiders目录下增加代码文件demo.py；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤3**：配置产生的spider爬虫\n",
    "- 修改spiders/demo.py文件\n",
    "- 配置初始URL地址\n",
    "- 配置获取页面后的解析方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤4**：运行爬虫，获取网页\n",
    "- 进入工程目录，执行如下命令：**>scrapy crawl demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy爬虫的数据类型\n",
    "- Request类\n",
    "    - class scrapy.http.Request()\n",
    "    - Request对象表示一个HTTP请求，由Spider生成，Downloader执行\n",
    "- Response类\n",
    "   - class scrapy.http.Response()\n",
    "   - Response对象表示一个HTTP响应，由Downloader生成，由Spider处理\n",
    "- Item类\n",
    "   - class scrapy.item.Item()\n",
    "   - Item对象表示一个从HTML页面中提取的信息内容，由Spider生成，由Item Pipeline处理\n",
    "   - Item类似字典类型，可以按照字典类型操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request类\n",
    "\n",
    "属性或方法 | 说明 \n",
    "---|---|\n",
    ".url  | Request对应的请求URL地址\n",
    ".method  | 对应的请求方法，'GET','POST'等\n",
    ".headers  | 字典类型风格的请求头\n",
    ".body | 请求内容主体，字符串类型\n",
    ".meta | 用户添加的扩展信息，在Scrapy内部模块间传递信息使用\n",
    ".copy() | 复制该请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response类\n",
    "\n",
    "属性或方法 | 说明 \n",
    "---|---|\n",
    ".url  | Response对应的URL地址\n",
    ".status  | HTTP状态码，默认是200\n",
    ".headers  | Response对应的头部信息\n",
    ".body | Response对应的内容信息，字符串类型\n",
    ".flags | 一组标记 \n",
    ".request | 产生Response类型对应的Request对象\n",
    ".copy() | 复制该响应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy 爬虫提取信息的方法\n",
    "\n",
    "Scrapy爬虫支持多种信息提取方法\n",
    "- Beautiful Soup\n",
    "- lxml\n",
    "- re\n",
    "- 选择器(selector)\n",
    "   - XPath Selector：XPath是一门在XML文档中查找信息的语言。\n",
    "   - CSS Selector：CSS(Cascading Style Sheets,层叠样式表)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例1： 天气预报Scrapy爬虫\n",
    "\n",
    "目标：获取某一城市一周天气预报，将结果保存为文本文件\n",
    "\n",
    "候选数据网站：http://www.tianqi.com\n",
    "\n",
    "\n",
    "功能描述：\n",
    "- 输入：天气数据网站\n",
    "- 输出：某一城市一周天气预报（日期，星期，天气，风向，最低温和最高温等）\n",
    "- 技术路线：Scrapy框架，bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫步骤\n",
    "\n",
    "步骤1：\n",
    "- \\>scrapy startproject weather\n",
    "- \\> cd weather\n",
    "- \\> scrapy genspider CityWeather tianqi.com\n",
    "- 进一步修改工程目录weather下面spiders目录下的CityWeather.py文件\n",
    "\n",
    "步骤2：\n",
    "- 配置stocks.py文件；\n",
    "- 修改对返回页面的处理；\n",
    "- 修改对新增URL爬取请求的处理。\n",
    "\n",
    "步骤3：\n",
    "- 修改工程目录weather下面的pipelines.py文件；\n",
    "- 定义对爬取项（Scraped item）的处理类；\n",
    "- 配置工程目录weather下面的settings.py文件，将其中的ITEM_PIPELINES的键改为pipelines.py中的处理类名。\n",
    "\n",
    "步骤4：\n",
    "- 运行爬虫，执行命令 - \\>scrapy crawl CityWeather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例2： 股票数据Scrapy爬虫\n",
    "\n",
    "目标：获取上交所和深交所所有股票的名称和交易信息\n",
    "\n",
    "候选数据网站：\n",
    "- 新浪股票：http://finance.sina.com.cn/stock/\n",
    "- 东方财富网：http://quote.eastmoney.com/stocklist.html\n",
    "- 股城网：https://hq.gucheng.com/gpdmylb.html\n",
    "\n",
    "如何选择数据：\n",
    "- 选取原则：股票信息静态存在于HTML页面中，非js代码生成，没有robots协议限制；\n",
    "- 选取方法：浏览器F12,查看源代码等；\n",
    "- 选取心态：不要纠结于某个网站，多找信息源尝试\n",
    "\n",
    "功能描述：\n",
    "- 输入：股票数据网站\n",
    "- 输出：沪深股票消息（每只股票的名称，最高，最低，昨收，涨停，成交量等）\n",
    "- 技术路线：Scrapy框架，re，bs4\n",
    "\n",
    "程序的结构设计：\n",
    "1. 从股城网获取股票列表\n",
    "2. 根据股票列表逐个到股城网获取个股信息\n",
    "3. 将结果存储到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
