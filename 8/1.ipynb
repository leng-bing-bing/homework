{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fc9854",
   "metadata": {},
   "source": [
    "# 实验8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b47f4",
   "metadata": {},
   "source": [
    "## 这次实验不太会，借鉴了答案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d2a8d",
   "metadata": {},
   "source": [
    "### 练习1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71ebb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ar       1.00      1.00      1.00        29\n",
      "          de       1.00      1.00      1.00        83\n",
      "          en       0.99      1.00      0.99        72\n",
      "          es       1.00      0.98      0.99        60\n",
      "          fr       0.99      0.99      0.99        71\n",
      "          it       0.98      0.98      0.98        43\n",
      "          ja       1.00      1.00      1.00        41\n",
      "          nl       1.00      1.00      1.00        19\n",
      "          pl       1.00      1.00      1.00        35\n",
      "          pt       1.00      1.00      1.00        45\n",
      "          ru       1.00      1.00      1.00        32\n",
      "\n",
      "    accuracy                           0.99       530\n",
      "   macro avg       1.00      1.00      1.00       530\n",
      "weighted avg       0.99      0.99      0.99       530\n",
      "\n",
      "[[29  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 83  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 72  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 59  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 70  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  0 42  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 41  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 35  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 45  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 32]]\n",
      "The language of \"This is a language detection test.\" is \"en\"\n",
      "The language of \"Ceci est un test de détection de la langue.\" is \"fr\"\n",
      "The language of \"Dies ist ein Test, um die Sprache zu erkennen.\" is \"de\"\n",
      "The language of \"これは言語検出テストです.\" is \"ja\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#加载数据\n",
    "# languages_data_folder = sys.argv[1]\n",
    "dataset = load_files('lan/paragraphs/')\n",
    "\n",
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.5)\n",
    "\n",
    "\n",
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char',\n",
    "                             use_idf=False)\n",
    "\n",
    "# TASK: Build a vectorizer / classifier pipeline using the previous analyzer\n",
    "# the pipeline instance should stored in a variable named clf\n",
    "clf = Pipeline([\n",
    "    ('vec', vectorizer),\n",
    "    ('clf', Perceptron()),\n",
    "])\n",
    "\n",
    "# TASK: Fit the pipeline on the training set\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = clf.predict(docs_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "#import matlotlib.pyplot as plt\n",
    "#plt.matshow(cm, cmap=plt.cm.jet)\n",
    "#plt.show()\n",
    "\n",
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    'This is a language detection test.',\n",
    "    'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "    'これは言語検出テストです.'\n",
    "]\n",
    "predicted = clf.predict(sentences)\n",
    "\n",
    "for s, p in zip(sentences, predicted):\n",
    "    print('The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c9284",
   "metadata": {},
   "source": [
    "### 练习2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e733e521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huanghj/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/huanghj/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/huanghj/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/huanghj/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/huanghj/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.02\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.86; std - 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.86      0.87       265\n",
      "         pos       0.84      0.86      0.85       235\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.86      0.86      0.86       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "[[227  38]\n",
      " [ 32 203]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # the training data folder must be passed as first argument\n",
    "#     movie_reviews_data_folder = sys.argv[1]\n",
    "    dataset = load_files('txt_sentoken', shuffle=False)\n",
    "    print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])\n",
    "\n",
    "  \n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "    grid_search.fit(docs_train, y_train)\n",
    "\n",
    "    # TASK: print the mean and std for each candidate along with the parameter\n",
    "    # settings for all the candidates explored by grid search.\n",
    "    n_candidates = len(grid_search.cv_results_['params'])\n",
    "    for i in range(n_candidates):\n",
    "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "                 % (grid_search.cv_results_['params'][i],\n",
    "                    grid_search.cv_results_['mean_test_score'][i],\n",
    "                    grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "    y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f93d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
